{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdpPHz2Lp6VD"
      },
      "source": [
        "# TP Coding autoencoders and variational autoencoders in Pytorch\n",
        "\n",
        "\n",
        "Author : Alasdair Newson\n",
        "\n",
        "alasdair.newson@telecom-paris.fr\n",
        "\n",
        "## Objective:\n",
        "\n",
        "The goal of this TP is to explore autoencoders and variational autoencoders applied to a simple dataset. In this first part, we will look at an autoencoder applied to MNIST. We recall that an autoencoder is a neural network with the following general architecture:\n",
        "\n",
        "\n",
        "![AUTOENCODER](https://perso.telecom-paristech.fr/anewson/doc/images/autoencoder_illustration_2.png)\n",
        "\n",
        "The tensor $z$ in the middle of the network is called a __latent code__, and it belongs to the latent space. It is this latent space which is interesting in autoencoders (for image synthesis, editing, etc).\n",
        "\n",
        "### Your task:\n",
        "You need to add the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE or # FILL IN CODE)\n",
        "\n",
        "\n",
        "First of all, let's load some packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqNeIJ8Op8Ao"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import pdb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def pytorch_to_numpy(x):\n",
        "  return x.detach().numpy()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyj5dj_eui9D"
      },
      "source": [
        "First, we load the mnist dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YPLKlPrufSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dba6873-3524-40b0-a2a5-01aacb2ec43e"
      },
      "source": [
        "\n",
        "batch_size = 128\n",
        "\n",
        "# MNIST Dataset\n",
        "mnist_trainset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "mnist_testset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "#create data loader with smaller dataset size\n",
        "max_mnist_size = 1000\n",
        "mnist_trainset_reduced = torch.utils.data.random_split(mnist_trainset, [max_mnist_size, len(mnist_trainset)-max_mnist_size])[0] \n",
        "mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset_reduced, batch_size=batch_size, shuffle=True,drop_last=True)\n",
        "\n",
        "# download test dataset\n",
        "max_mnist_size = 512\n",
        "mnist_testset_reduced = torch.utils.data.random_split(mnist_testset, [max_mnist_size, len(mnist_testset)-max_mnist_size])[0] \n",
        "mnist_test_loader = torch.utils.data.DataLoader(mnist_testset_reduced, batch_size=batch_size, shuffle=True,drop_last=True)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 313606173.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 21383176.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 111223727.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 23606603.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_trainset_reduced.dataset.train_data.shape"
      ],
      "metadata": {
        "id": "r7YhlBT2PN9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f0b253-66ca-4282-c5f5-12f3f02b7aa6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-bkK4ktwfvC"
      },
      "source": [
        "# 1 Vanilla Autoencoder\n",
        "\n",
        "Now, we define the general parameters of the autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD56EDzbvUxq"
      },
      "source": [
        "# autoencoder parameters\n",
        "n_rows = mnist_trainset_reduced.dataset.train_data.shape[1]\n",
        "n_cols = mnist_trainset_reduced.dataset.train_data.shape[2]\n",
        "n_channels = 1\n",
        "n_pixels = n_rows*n_cols\n",
        "\n",
        "img_shape = (n_rows, n_cols, n_channels)\n",
        "n_epochs = 150"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jLa2-jQwxSI"
      },
      "source": [
        "Now, define the autoencoder architecture. In the first part, we will use the following MLP architecture :\n",
        "\n",
        "Encoder :\n",
        "- Flatten input\n",
        "- Dense layer, output size h_dim_1 + ReLU\n",
        "- Dense layer, output size h_dim_2 + ReLU\n",
        "- Dense layer, output size z_dim (no non-linearity)\n",
        "\n",
        "Decoder :\n",
        "- Dense layer, output size h_dim_2 + ReLU\n",
        "- Dense layer, output size h_dim_1 + ReLU\n",
        "- Dense layer, output size x_dim + Sigmoid Activation\n",
        "- Reshape, to size $28\\times 28\\times 1$\n",
        "\n",
        "For the Reshape operation, use the ```A.view(dim_1,dim_2,...)``` function, where ```A``` is your tensor."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AE(torch.nn.Module):\n",
        "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, n_rows, n_cols, n_channels):\n",
        "        super(AE, self).__init__()\n",
        "\n",
        "        self.n_rows = n_rows\n",
        "        self.n_cols = n_cols\n",
        "        self.n_channels = n_channels\n",
        "        self.n_pixels = (self.n_rows)*(self.n_cols)\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # encoder part\n",
        "        self.fc1 = torch.nn.Linear(self.n_pixels*self.n_channels, h_dim1)\n",
        "        self.fc2 = torch.nn.Linear(h_dim1, h_dim2)\n",
        "        self.fc3 = torch.nn.Linear(h_dim2, z_dim)\n",
        "        # decoder part\n",
        "        self.fc4 = torch.nn.Linear(z_dim, h_dim2)\n",
        "        self.fc5 = torch.nn.Linear(h_dim2, h_dim1)\n",
        "        self.fc6 = torch.nn.Linear(h_dim1, self.n_pixels*self.n_channels)\n",
        "\n",
        "    def encoder(self, x):\n",
        "        x = x.view(-1, self.n_pixels*self.n_channels)\n",
        "        h = torch.relu(self.fc1(x))\n",
        "        h = torch.relu(self.fc2(h))\n",
        "        z = self.fc3(h)\n",
        "        return z\n",
        "\n",
        "    def decoder(self, z):\n",
        "        h = torch.relu(self.fc4(z))\n",
        "        h = torch.relu(self.fc5(h))\n",
        "        x_hat = torch.sigmoid(self.fc6(h))\n",
        "        x_hat = x_hat.view(-1, self.n_channels, self.n_rows, self.n_cols)\n",
        "        return x_hat\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        return x_hat\n",
        "\n",
        "    def loss_function(self, x, x_hat):\n",
        "        bce_loss = torch.nn.functional.binary_cross_entropy(x_hat, x, reduction='none')\n",
        "        bce_loss = torch.sum(bce_loss, dim=(1,2,3))\n",
        "        return torch.mean(bce_loss)\n"
      ],
      "metadata": {
        "id": "tEuZfnUlXxMl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model\n",
        "ae_dim_1 = 512\n",
        "ae_dim_2 = 256\n",
        "z_dim = 10\n",
        "ae_model = AE(x_dim=n_pixels, h_dim1= ae_dim_1, h_dim2=ae_dim_2, z_dim=z_dim,n_rows=n_rows,n_cols=n_cols,n_channels=n_channels)\n",
        "ae_optimizer = optim.Adam(ae_model.parameters())"
      ],
      "metadata": {
        "id": "oV40vRMQRoG1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-659aM36xvXX"
      },
      "source": [
        "Now, define a generic function to train the model for one epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfqX6Brlxjyi"
      },
      "source": [
        "def train_ae(ae_model,data_train_loader,epoch):\n",
        "\ttrain_loss = 0\n",
        "\tfor batch_idx, (data, _) in enumerate(data_train_loader):\n",
        "\t\tae_optimizer.zero_grad()\n",
        "\t\t\n",
        "\t\ty = ae_model.forward(data) # FILL IN CODE HERE\n",
        "\t\tloss_ae = ae_model.loss_function(data, y) # FILL IN CODE HERE\n",
        "  \n",
        "\t\tloss_ae.backward()\n",
        "\t\ttrain_loss += loss_ae.item()\n",
        "\t\tae_optimizer.step()\n",
        "\t\t\n",
        "\t\tif batch_idx % 100 == 0:\n",
        "\t\t\tprint('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "\t\t\t\tepoch, batch_idx * len(data), len(data_train_loader.dataset),\n",
        "\t\t\t\t100. * batch_idx / len(data_train_loader), loss_ae.item() / len(data)))\n",
        "\tprint('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(data_train_loader.dataset)))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3EbmswSzJdK"
      },
      "source": [
        "We define a function to carry out testing on the autoencoder model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(0, n_epochs):\n",
        "  train_ae(ae_model,mnist_train_loader,epoch)"
      ],
      "metadata": {
        "id": "11q_0PSibZk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20eba2f4-310b-48ba-e40d-571f5f845eae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/1000 (0%)]\tLoss: 4.251325\n",
            "====> Epoch: 0 Average loss: 3.4330\n",
            "Train Epoch: 1 [0/1000 (0%)]\tLoss: 2.514873\n",
            "====> Epoch: 1 Average loss: 1.9380\n",
            "Train Epoch: 2 [0/1000 (0%)]\tLoss: 1.757986\n",
            "====> Epoch: 2 Average loss: 1.6335\n",
            "Train Epoch: 3 [0/1000 (0%)]\tLoss: 1.764185\n",
            "====> Epoch: 3 Average loss: 1.5250\n",
            "Train Epoch: 4 [0/1000 (0%)]\tLoss: 1.667119\n",
            "====> Epoch: 4 Average loss: 1.4766\n",
            "Train Epoch: 5 [0/1000 (0%)]\tLoss: 1.649221\n",
            "====> Epoch: 5 Average loss: 1.4516\n",
            "Train Epoch: 6 [0/1000 (0%)]\tLoss: 1.666181\n",
            "====> Epoch: 6 Average loss: 1.4350\n",
            "Train Epoch: 7 [0/1000 (0%)]\tLoss: 1.586708\n",
            "====> Epoch: 7 Average loss: 1.4151\n",
            "Train Epoch: 8 [0/1000 (0%)]\tLoss: 1.537609\n",
            "====> Epoch: 8 Average loss: 1.3955\n",
            "Train Epoch: 9 [0/1000 (0%)]\tLoss: 1.548398\n",
            "====> Epoch: 9 Average loss: 1.3980\n",
            "Train Epoch: 10 [0/1000 (0%)]\tLoss: 1.623060\n",
            "====> Epoch: 10 Average loss: 1.3721\n",
            "Train Epoch: 11 [0/1000 (0%)]\tLoss: 1.452459\n",
            "====> Epoch: 11 Average loss: 1.3495\n",
            "Train Epoch: 12 [0/1000 (0%)]\tLoss: 1.509955\n",
            "====> Epoch: 12 Average loss: 1.3295\n",
            "Train Epoch: 13 [0/1000 (0%)]\tLoss: 1.433899\n",
            "====> Epoch: 13 Average loss: 1.2989\n",
            "Train Epoch: 14 [0/1000 (0%)]\tLoss: 1.415010\n",
            "====> Epoch: 14 Average loss: 1.2811\n",
            "Train Epoch: 15 [0/1000 (0%)]\tLoss: 1.432199\n",
            "====> Epoch: 15 Average loss: 1.2610\n",
            "Train Epoch: 16 [0/1000 (0%)]\tLoss: 1.404250\n",
            "====> Epoch: 16 Average loss: 1.2405\n",
            "Train Epoch: 17 [0/1000 (0%)]\tLoss: 1.349354\n",
            "====> Epoch: 17 Average loss: 1.2112\n",
            "Train Epoch: 18 [0/1000 (0%)]\tLoss: 1.366634\n",
            "====> Epoch: 18 Average loss: 1.1798\n",
            "Train Epoch: 19 [0/1000 (0%)]\tLoss: 1.315717\n",
            "====> Epoch: 19 Average loss: 1.1446\n",
            "Train Epoch: 20 [0/1000 (0%)]\tLoss: 1.296252\n",
            "====> Epoch: 20 Average loss: 1.1202\n",
            "Train Epoch: 21 [0/1000 (0%)]\tLoss: 1.261810\n",
            "====> Epoch: 21 Average loss: 1.0891\n",
            "Train Epoch: 22 [0/1000 (0%)]\tLoss: 1.192167\n",
            "====> Epoch: 22 Average loss: 1.0535\n",
            "Train Epoch: 23 [0/1000 (0%)]\tLoss: 1.129842\n",
            "====> Epoch: 23 Average loss: 1.0222\n",
            "Train Epoch: 24 [0/1000 (0%)]\tLoss: 1.095851\n",
            "====> Epoch: 24 Average loss: 0.9918\n",
            "Train Epoch: 25 [0/1000 (0%)]\tLoss: 1.109841\n",
            "====> Epoch: 25 Average loss: 0.9710\n",
            "Train Epoch: 26 [0/1000 (0%)]\tLoss: 1.020033\n",
            "====> Epoch: 26 Average loss: 0.9550\n",
            "Train Epoch: 27 [0/1000 (0%)]\tLoss: 1.094217\n",
            "====> Epoch: 27 Average loss: 0.9364\n",
            "Train Epoch: 28 [0/1000 (0%)]\tLoss: 1.009830\n",
            "====> Epoch: 28 Average loss: 0.9164\n",
            "Train Epoch: 29 [0/1000 (0%)]\tLoss: 0.989132\n",
            "====> Epoch: 29 Average loss: 0.9013\n",
            "Train Epoch: 30 [0/1000 (0%)]\tLoss: 1.058849\n",
            "====> Epoch: 30 Average loss: 0.8823\n",
            "Train Epoch: 31 [0/1000 (0%)]\tLoss: 0.949139\n",
            "====> Epoch: 31 Average loss: 0.8636\n",
            "Train Epoch: 32 [0/1000 (0%)]\tLoss: 0.959946\n",
            "====> Epoch: 32 Average loss: 0.8499\n",
            "Train Epoch: 33 [0/1000 (0%)]\tLoss: 0.943145\n",
            "====> Epoch: 33 Average loss: 0.8421\n",
            "Train Epoch: 34 [0/1000 (0%)]\tLoss: 0.908940\n",
            "====> Epoch: 34 Average loss: 0.8226\n",
            "Train Epoch: 35 [0/1000 (0%)]\tLoss: 0.937574\n",
            "====> Epoch: 35 Average loss: 0.8176\n",
            "Train Epoch: 36 [0/1000 (0%)]\tLoss: 0.907656\n",
            "====> Epoch: 36 Average loss: 0.8091\n",
            "Train Epoch: 37 [0/1000 (0%)]\tLoss: 0.862622\n",
            "====> Epoch: 37 Average loss: 0.7990\n",
            "Train Epoch: 38 [0/1000 (0%)]\tLoss: 0.931221\n",
            "====> Epoch: 38 Average loss: 0.7908\n",
            "Train Epoch: 39 [0/1000 (0%)]\tLoss: 0.900744\n",
            "====> Epoch: 39 Average loss: 0.7843\n",
            "Train Epoch: 40 [0/1000 (0%)]\tLoss: 0.852963\n",
            "====> Epoch: 40 Average loss: 0.7712\n",
            "Train Epoch: 41 [0/1000 (0%)]\tLoss: 0.826761\n",
            "====> Epoch: 41 Average loss: 0.7664\n",
            "Train Epoch: 42 [0/1000 (0%)]\tLoss: 0.873003\n",
            "====> Epoch: 42 Average loss: 0.7636\n",
            "Train Epoch: 43 [0/1000 (0%)]\tLoss: 0.840626\n",
            "====> Epoch: 43 Average loss: 0.7532\n",
            "Train Epoch: 44 [0/1000 (0%)]\tLoss: 0.811094\n",
            "====> Epoch: 44 Average loss: 0.7458\n",
            "Train Epoch: 45 [0/1000 (0%)]\tLoss: 0.794801\n",
            "====> Epoch: 45 Average loss: 0.7354\n",
            "Train Epoch: 46 [0/1000 (0%)]\tLoss: 0.820242\n",
            "====> Epoch: 46 Average loss: 0.7325\n",
            "Train Epoch: 47 [0/1000 (0%)]\tLoss: 0.780267\n",
            "====> Epoch: 47 Average loss: 0.7237\n",
            "Train Epoch: 48 [0/1000 (0%)]\tLoss: 0.778683\n",
            "====> Epoch: 48 Average loss: 0.7162\n",
            "Train Epoch: 49 [0/1000 (0%)]\tLoss: 0.786431\n",
            "====> Epoch: 49 Average loss: 0.7136\n",
            "Train Epoch: 50 [0/1000 (0%)]\tLoss: 0.803800\n",
            "====> Epoch: 50 Average loss: 0.7082\n",
            "Train Epoch: 51 [0/1000 (0%)]\tLoss: 0.751912\n",
            "====> Epoch: 51 Average loss: 0.7000\n",
            "Train Epoch: 52 [0/1000 (0%)]\tLoss: 0.776361\n",
            "====> Epoch: 52 Average loss: 0.6961\n",
            "Train Epoch: 53 [0/1000 (0%)]\tLoss: 0.766171\n",
            "====> Epoch: 53 Average loss: 0.6883\n",
            "Train Epoch: 54 [0/1000 (0%)]\tLoss: 0.785032\n",
            "====> Epoch: 54 Average loss: 0.6888\n",
            "Train Epoch: 55 [0/1000 (0%)]\tLoss: 0.783391\n",
            "====> Epoch: 55 Average loss: 0.6847\n",
            "Train Epoch: 56 [0/1000 (0%)]\tLoss: 0.709146\n",
            "====> Epoch: 56 Average loss: 0.6781\n",
            "Train Epoch: 57 [0/1000 (0%)]\tLoss: 0.786722\n",
            "====> Epoch: 57 Average loss: 0.6751\n",
            "Train Epoch: 58 [0/1000 (0%)]\tLoss: 0.747449\n",
            "====> Epoch: 58 Average loss: 0.6709\n",
            "Train Epoch: 59 [0/1000 (0%)]\tLoss: 0.740616\n",
            "====> Epoch: 59 Average loss: 0.6667\n",
            "Train Epoch: 60 [0/1000 (0%)]\tLoss: 0.735668\n",
            "====> Epoch: 60 Average loss: 0.6587\n",
            "Train Epoch: 61 [0/1000 (0%)]\tLoss: 0.711962\n",
            "====> Epoch: 61 Average loss: 0.6550\n",
            "Train Epoch: 62 [0/1000 (0%)]\tLoss: 0.702180\n",
            "====> Epoch: 62 Average loss: 0.6475\n",
            "Train Epoch: 63 [0/1000 (0%)]\tLoss: 0.729261\n",
            "====> Epoch: 63 Average loss: 0.6461\n",
            "Train Epoch: 64 [0/1000 (0%)]\tLoss: 0.727511\n",
            "====> Epoch: 64 Average loss: 0.6454\n",
            "Train Epoch: 65 [0/1000 (0%)]\tLoss: 0.707334\n",
            "====> Epoch: 65 Average loss: 0.6384\n",
            "Train Epoch: 66 [0/1000 (0%)]\tLoss: 0.728721\n",
            "====> Epoch: 66 Average loss: 0.6371\n",
            "Train Epoch: 67 [0/1000 (0%)]\tLoss: 0.710517\n",
            "====> Epoch: 67 Average loss: 0.6326\n",
            "Train Epoch: 68 [0/1000 (0%)]\tLoss: 0.720250\n",
            "====> Epoch: 68 Average loss: 0.6319\n",
            "Train Epoch: 69 [0/1000 (0%)]\tLoss: 0.661623\n",
            "====> Epoch: 69 Average loss: 0.6217\n",
            "Train Epoch: 70 [0/1000 (0%)]\tLoss: 0.690500\n",
            "====> Epoch: 70 Average loss: 0.6245\n",
            "Train Epoch: 71 [0/1000 (0%)]\tLoss: 0.674426\n",
            "====> Epoch: 71 Average loss: 0.6164\n",
            "Train Epoch: 72 [0/1000 (0%)]\tLoss: 0.682710\n",
            "====> Epoch: 72 Average loss: 0.6086\n",
            "Train Epoch: 73 [0/1000 (0%)]\tLoss: 0.684035\n",
            "====> Epoch: 73 Average loss: 0.6100\n",
            "Train Epoch: 74 [0/1000 (0%)]\tLoss: 0.658003\n",
            "====> Epoch: 74 Average loss: 0.6047\n",
            "Train Epoch: 75 [0/1000 (0%)]\tLoss: 0.670846\n",
            "====> Epoch: 75 Average loss: 0.6051\n",
            "Train Epoch: 76 [0/1000 (0%)]\tLoss: 0.659966\n",
            "====> Epoch: 76 Average loss: 0.5951\n",
            "Train Epoch: 77 [0/1000 (0%)]\tLoss: 0.643954\n",
            "====> Epoch: 77 Average loss: 0.5926\n",
            "Train Epoch: 78 [0/1000 (0%)]\tLoss: 0.671534\n",
            "====> Epoch: 78 Average loss: 0.5930\n",
            "Train Epoch: 79 [0/1000 (0%)]\tLoss: 0.647595\n",
            "====> Epoch: 79 Average loss: 0.5893\n",
            "Train Epoch: 80 [0/1000 (0%)]\tLoss: 0.647666\n",
            "====> Epoch: 80 Average loss: 0.5843\n",
            "Train Epoch: 81 [0/1000 (0%)]\tLoss: 0.640397\n",
            "====> Epoch: 81 Average loss: 0.5817\n",
            "Train Epoch: 82 [0/1000 (0%)]\tLoss: 0.651854\n",
            "====> Epoch: 82 Average loss: 0.5775\n",
            "Train Epoch: 83 [0/1000 (0%)]\tLoss: 0.631316\n",
            "====> Epoch: 83 Average loss: 0.5727\n",
            "Train Epoch: 84 [0/1000 (0%)]\tLoss: 0.637185\n",
            "====> Epoch: 84 Average loss: 0.5736\n",
            "Train Epoch: 85 [0/1000 (0%)]\tLoss: 0.635213\n",
            "====> Epoch: 85 Average loss: 0.5715\n",
            "Train Epoch: 86 [0/1000 (0%)]\tLoss: 0.633259\n",
            "====> Epoch: 86 Average loss: 0.5663\n",
            "Train Epoch: 87 [0/1000 (0%)]\tLoss: 0.626435\n",
            "====> Epoch: 87 Average loss: 0.5639\n",
            "Train Epoch: 88 [0/1000 (0%)]\tLoss: 0.603895\n",
            "====> Epoch: 88 Average loss: 0.5580\n",
            "Train Epoch: 89 [0/1000 (0%)]\tLoss: 0.605603\n",
            "====> Epoch: 89 Average loss: 0.5593\n",
            "Train Epoch: 90 [0/1000 (0%)]\tLoss: 0.597967\n",
            "====> Epoch: 90 Average loss: 0.5562\n",
            "Train Epoch: 91 [0/1000 (0%)]\tLoss: 0.610663\n",
            "====> Epoch: 91 Average loss: 0.5543\n",
            "Train Epoch: 92 [0/1000 (0%)]\tLoss: 0.601410\n",
            "====> Epoch: 92 Average loss: 0.5504\n",
            "Train Epoch: 93 [0/1000 (0%)]\tLoss: 0.588690\n",
            "====> Epoch: 93 Average loss: 0.5500\n",
            "Train Epoch: 94 [0/1000 (0%)]\tLoss: 0.608618\n",
            "====> Epoch: 94 Average loss: 0.5459\n",
            "Train Epoch: 95 [0/1000 (0%)]\tLoss: 0.621270\n",
            "====> Epoch: 95 Average loss: 0.5445\n",
            "Train Epoch: 96 [0/1000 (0%)]\tLoss: 0.602599\n",
            "====> Epoch: 96 Average loss: 0.5406\n",
            "Train Epoch: 97 [0/1000 (0%)]\tLoss: 0.603828\n",
            "====> Epoch: 97 Average loss: 0.5364\n",
            "Train Epoch: 98 [0/1000 (0%)]\tLoss: 0.599058\n",
            "====> Epoch: 98 Average loss: 0.5420\n",
            "Train Epoch: 99 [0/1000 (0%)]\tLoss: 0.617552\n",
            "====> Epoch: 99 Average loss: 0.5356\n",
            "Train Epoch: 100 [0/1000 (0%)]\tLoss: 0.574007\n",
            "====> Epoch: 100 Average loss: 0.5366\n",
            "Train Epoch: 101 [0/1000 (0%)]\tLoss: 0.574408\n",
            "====> Epoch: 101 Average loss: 0.5320\n",
            "Train Epoch: 102 [0/1000 (0%)]\tLoss: 0.607564\n",
            "====> Epoch: 102 Average loss: 0.5337\n",
            "Train Epoch: 103 [0/1000 (0%)]\tLoss: 0.565776\n",
            "====> Epoch: 103 Average loss: 0.5313\n",
            "Train Epoch: 104 [0/1000 (0%)]\tLoss: 0.558584\n",
            "====> Epoch: 104 Average loss: 0.5245\n",
            "Train Epoch: 105 [0/1000 (0%)]\tLoss: 0.570646\n",
            "====> Epoch: 105 Average loss: 0.5225\n",
            "Train Epoch: 106 [0/1000 (0%)]\tLoss: 0.596015\n",
            "====> Epoch: 106 Average loss: 0.5199\n",
            "Train Epoch: 107 [0/1000 (0%)]\tLoss: 0.580716\n",
            "====> Epoch: 107 Average loss: 0.5223\n",
            "Train Epoch: 108 [0/1000 (0%)]\tLoss: 0.589673\n",
            "====> Epoch: 108 Average loss: 0.5177\n",
            "Train Epoch: 109 [0/1000 (0%)]\tLoss: 0.566178\n",
            "====> Epoch: 109 Average loss: 0.5127\n",
            "Train Epoch: 110 [0/1000 (0%)]\tLoss: 0.574274\n",
            "====> Epoch: 110 Average loss: 0.5130\n",
            "Train Epoch: 111 [0/1000 (0%)]\tLoss: 0.573655\n",
            "====> Epoch: 111 Average loss: 0.5113\n",
            "Train Epoch: 112 [0/1000 (0%)]\tLoss: 0.561100\n",
            "====> Epoch: 112 Average loss: 0.5075\n",
            "Train Epoch: 113 [0/1000 (0%)]\tLoss: 0.581936\n",
            "====> Epoch: 113 Average loss: 0.5068\n",
            "Train Epoch: 114 [0/1000 (0%)]\tLoss: 0.545087\n",
            "====> Epoch: 114 Average loss: 0.5075\n",
            "Train Epoch: 115 [0/1000 (0%)]\tLoss: 0.566270\n",
            "====> Epoch: 115 Average loss: 0.5076\n",
            "Train Epoch: 116 [0/1000 (0%)]\tLoss: 0.552609\n",
            "====> Epoch: 116 Average loss: 0.5028\n",
            "Train Epoch: 117 [0/1000 (0%)]\tLoss: 0.551630\n",
            "====> Epoch: 117 Average loss: 0.5020\n",
            "Train Epoch: 118 [0/1000 (0%)]\tLoss: 0.545896\n",
            "====> Epoch: 118 Average loss: 0.4996\n",
            "Train Epoch: 119 [0/1000 (0%)]\tLoss: 0.551544\n",
            "====> Epoch: 119 Average loss: 0.4976\n",
            "Train Epoch: 120 [0/1000 (0%)]\tLoss: 0.557903\n",
            "====> Epoch: 120 Average loss: 0.4965\n",
            "Train Epoch: 121 [0/1000 (0%)]\tLoss: 0.554906\n",
            "====> Epoch: 121 Average loss: 0.4951\n",
            "Train Epoch: 122 [0/1000 (0%)]\tLoss: 0.555397\n",
            "====> Epoch: 122 Average loss: 0.4925\n",
            "Train Epoch: 123 [0/1000 (0%)]\tLoss: 0.535677\n",
            "====> Epoch: 123 Average loss: 0.4934\n",
            "Train Epoch: 124 [0/1000 (0%)]\tLoss: 0.558107\n",
            "====> Epoch: 124 Average loss: 0.4927\n",
            "Train Epoch: 125 [0/1000 (0%)]\tLoss: 0.558057\n",
            "====> Epoch: 125 Average loss: 0.4920\n",
            "Train Epoch: 126 [0/1000 (0%)]\tLoss: 0.536425\n",
            "====> Epoch: 126 Average loss: 0.4896\n",
            "Train Epoch: 127 [0/1000 (0%)]\tLoss: 0.530773\n",
            "====> Epoch: 127 Average loss: 0.4886\n",
            "Train Epoch: 128 [0/1000 (0%)]\tLoss: 0.519357\n",
            "====> Epoch: 128 Average loss: 0.4819\n",
            "Train Epoch: 129 [0/1000 (0%)]\tLoss: 0.521513\n",
            "====> Epoch: 129 Average loss: 0.4820\n",
            "Train Epoch: 130 [0/1000 (0%)]\tLoss: 0.530240\n",
            "====> Epoch: 130 Average loss: 0.4798\n",
            "Train Epoch: 131 [0/1000 (0%)]\tLoss: 0.530713\n",
            "====> Epoch: 131 Average loss: 0.4784\n",
            "Train Epoch: 132 [0/1000 (0%)]\tLoss: 0.535265\n",
            "====> Epoch: 132 Average loss: 0.4767\n",
            "Train Epoch: 133 [0/1000 (0%)]\tLoss: 0.524854\n",
            "====> Epoch: 133 Average loss: 0.4764\n",
            "Train Epoch: 134 [0/1000 (0%)]\tLoss: 0.534043\n",
            "====> Epoch: 134 Average loss: 0.4757\n",
            "Train Epoch: 135 [0/1000 (0%)]\tLoss: 0.511311\n",
            "====> Epoch: 135 Average loss: 0.4735\n",
            "Train Epoch: 136 [0/1000 (0%)]\tLoss: 0.537014\n",
            "====> Epoch: 136 Average loss: 0.4714\n",
            "Train Epoch: 137 [0/1000 (0%)]\tLoss: 0.521261\n",
            "====> Epoch: 137 Average loss: 0.4713\n",
            "Train Epoch: 138 [0/1000 (0%)]\tLoss: 0.524055\n",
            "====> Epoch: 138 Average loss: 0.4698\n",
            "Train Epoch: 139 [0/1000 (0%)]\tLoss: 0.545959\n",
            "====> Epoch: 139 Average loss: 0.4682\n",
            "Train Epoch: 140 [0/1000 (0%)]\tLoss: 0.548048\n",
            "====> Epoch: 140 Average loss: 0.4689\n",
            "Train Epoch: 141 [0/1000 (0%)]\tLoss: 0.516666\n",
            "====> Epoch: 141 Average loss: 0.4650\n",
            "Train Epoch: 142 [0/1000 (0%)]\tLoss: 0.540859\n",
            "====> Epoch: 142 Average loss: 0.4654\n",
            "Train Epoch: 143 [0/1000 (0%)]\tLoss: 0.528665\n",
            "====> Epoch: 143 Average loss: 0.4662\n",
            "Train Epoch: 144 [0/1000 (0%)]\tLoss: 0.483875\n",
            "====> Epoch: 144 Average loss: 0.4633\n",
            "Train Epoch: 145 [0/1000 (0%)]\tLoss: 0.502048\n",
            "====> Epoch: 145 Average loss: 0.4619\n",
            "Train Epoch: 146 [0/1000 (0%)]\tLoss: 0.508402\n",
            "====> Epoch: 146 Average loss: 0.4603\n",
            "Train Epoch: 147 [0/1000 (0%)]\tLoss: 0.501458\n",
            "====> Epoch: 147 Average loss: 0.4577\n",
            "Train Epoch: 148 [0/1000 (0%)]\tLoss: 0.504640\n",
            "====> Epoch: 148 Average loss: 0.4579\n",
            "Train Epoch: 149 [0/1000 (0%)]\tLoss: 0.499023\n",
            "====> Epoch: 149 Average loss: 0.4587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8jXjdRyzMy2"
      },
      "source": [
        "def display_images(imgs):\n",
        "  \n",
        "  r = 1\n",
        "  c = imgs.shape[0]\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  for j in range(c):\n",
        "    #black and white images\n",
        "    axs[j].imshow(pytorch_to_numpy(imgs[j, 0,:,:]), cmap='gray')\n",
        "    axs[j].axis('off')\n",
        "  plt.show()\n",
        "\n",
        "def display_ae_images(ae_model, test_imgs):\n",
        "  n_images = 5\n",
        "  idx = np.random.randint(0, test_imgs.shape[0], n_images)\n",
        "  test_imgs = test_imgs[idx,:,:,:]\n",
        "  print(test_imgs.shape)\n",
        "\n",
        "  #get output images\n",
        "  output_imgs = pytorch_to_numpy(ae_model.forward( test_imgs ))\n",
        "  print(output_imgs.shape)\n",
        "  \n",
        "  r = 2\n",
        "  c = n_images\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  for j in range(c):\n",
        "    #black and white images\n",
        "    axs[0,j].imshow(test_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[0,j].axis('off')\n",
        "    axs[1,j].imshow(output_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[1,j].axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_imgs = next(iter(mnist_train_loader))[0]\n",
        "display_ae_images(ae_model, test_imgs)"
      ],
      "metadata": {
        "id": "9pbXch29d68D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "8eee11e1-a0af-4a67-d548-055aac6a4a33"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 1, 28, 28])\n",
            "(5, 1, 28, 28)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Question__ Are you satisfied with the results, do they look good ?"
      ],
      "metadata": {
        "id": "assPaJqB5sa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Answer__\n",
        "Yes, I am quite satisfied with the results, as we can see we have in the second line images with lower definition but we can see that the numbers are still lisable."
      ],
      "metadata": {
        "id": "Q7uoWa4O5x8R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO9ATQEiyr3b"
      },
      "source": [
        "## 2/ Two simple generative models\n",
        "\n",
        "In this section, we consider two naïve approaches to creating generative models. The general idea is the following:\n",
        "\n",
        "- train an autoencoder\n",
        "- estimate different statistics (average, variance) of the data in the latent space\n",
        "- using these statistics, define a model based on a Gaussian distribution\n",
        "- generate data with this distribution\n",
        "\n",
        "We will consider these two situations :\n",
        "\n",
        "- a multivariate Gaussian distribution with __diagonal covariance matrix__ (each latent component is an independent random variable). This requires the average and variance in each latent component\n",
        "- a multivariate Gaussian distribution with __non-diagonal covariance matrix__. This requires the average and covariance matrix of the latent components\n",
        "\n",
        "Obviously, since this is done _a posteriori_ after training the autoencoder, there is nothing which guarantees that the latent codes do indeed follow a Gaussian distribution. Our goal will be to verify that Variational Autoencoders indeed produce better results than such naïve approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 Defining and generating random Gaussian latent codes\n",
        "\n",
        "Let $z$ be a latent code and $d$ the dimension of the latent space (called ``z_dim`` in the code). We suppose that the $z$'s follow a multivariate Gaussian distribution, written as:\n",
        "\n",
        "\\begin{equation}\n",
        "z \\sim \\mathcal{N}\\left(\n",
        "\\mu,\n",
        "\\bf{C}\n",
        "\\right),\n",
        "\\end{equation}\n",
        "where $\\mu$ and $\\bf{C}$ are the average vector and covariance matrix of the Gaussian distribution. To define such a generative model, we must therefore determine $\\mu$ and $\\bf{C}$. Once this is done, we can generate a random Gaussian latent code in the following manner:\n",
        "\n",
        "\\begin{equation}\n",
        "z = \\mu + {\\bf{L}} \\varepsilon,\n",
        "\\end{equation}\n",
        "where $\\varepsilon$ is a random vector drawn from a multivariate normal distribution ($\\mu=0$ and ${\\bf{C}} = \\text{Id}$), and $\\bf{L}$ is the Cholesky decomposition of the positive semi-definite covariance matrix. In other words:\n",
        "\n",
        "\\begin{equation}\n",
        "{\\bf{C}} = {\\bf{L}}{\\bf{L}^T}.\n",
        "\\end{equation}\n",
        "\n",
        "This gives a simple method of producing a multivariate Gaussian random variable."
      ],
      "metadata": {
        "id": "x2M1-BRmf56d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1/ A Gaussian model with diagonal covariance\n",
        "\n",
        "The first naïve model is  defined in this first case as:\n",
        "\n",
        "- $\\bf{\\mu}=\\left[\\mu_0, \\mu_1, \\cdots, \\mu_{d-1}\\right]^T$\n",
        "- $\n",
        "  \\bf{C} = \\begin{pmatrix}\n",
        "\\sigma_0^2 & 0 & \\cdots & 0 \\\\\n",
        "0 & \\sigma_1^2 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & \\sigma_{d-1}^2\n",
        "\\end{pmatrix}$\n",
        "\n",
        "In this situation, therefore, the matrix $\\bf{L}$ can be calculated quite simply, as:\n",
        "- $\n",
        "  \\bf{L} = \\begin{pmatrix}\n",
        "\\sigma_0 & 0 & \\cdots & 0 \\\\\n",
        "0 & \\sigma_1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & \\sigma_{d-1}\n",
        "\\end{pmatrix}$\n",
        "\n",
        "In the next cell, calculate the empirical average and variances over a certain number of batches:"
      ],
      "metadata": {
        "id": "NWpucm972i7j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUXHCtvW2iQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "988a0075-c320-4714-81cb-0e5a01bddf2c"
      },
      "source": [
        "n_batches = np.floor(len(mnist_train_loader.dataset.indices) / batch_size).astype(int)\n",
        "\n",
        "z_average = torch.zeros(n_batches, ae_model.z_dim)\n",
        "z_sigma = torch.zeros(n_batches, ae_model.z_dim)\n",
        "\n",
        "for batch_idx, (data, _) in enumerate(mnist_train_loader):\n",
        "    z = ae_model.encoder(data)\n",
        "    z_average[batch_idx, :] = z.mean(dim=0)\n",
        "    z_sigma[batch_idx, :] = z.std(dim=0)\n",
        "\n",
        "z_average = z_average.mean(dim=0)\n",
        "z_sigma = z_sigma.mean(dim=0)\n",
        "\n",
        "print(\"Average of latent codes:\", z_average)\n",
        "print(\"Standard deviation of latent codes:\", z_sigma)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average of latent codes: tensor([  4.0788,  -4.2869,   1.5684,  -1.3008,   1.6831,   5.3575, -10.4099,\n",
            "         -8.4990,   8.4451,   4.4062], grad_fn=<MeanBackward1>)\n",
            "Standard deviation of latent codes: tensor([7.5178, 9.4034, 8.3773, 8.5635, 8.4564, 7.6709, 9.0684, 7.7865, 7.0609,\n",
            "        5.4657], grad_fn=<MeanBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in the next cell generate data with this simple generative model using the approach described above. Display these images with the ``display_images`` function. \n",
        "\n",
        "__NB__ You do not actually have to define the matrix $\\bf{L}$ in this case, an element-wise multiplication of two (properly chosen) vectors will suffice. To generate multivariate normal random variables you can use the following Pytorch function:\n",
        "\n",
        "- ```torch.randn```\n"
      ],
      "metadata": {
        "id": "Lrpc62ML9K4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images_diagonal_gaussian(ae_model, z_average, z_sigma, n_images=5):\n",
        "    epsilon = torch.randn(n_images, ae_model.z_dim)\n",
        "    z_generated = epsilon * z_sigma + z_average\n",
        "    imgs_generated = ae_model.decoder(z_generated)\n",
        "    return imgs_generated\n",
        "\n",
        "imgs_generated = generate_images_diagonal_gaussian(ae_model, z_average, z_sigma, n_images=5)\n",
        "display_images(imgs_generated)\n"
      ],
      "metadata": {
        "id": "1_Tekii-9QEo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c33015d2-d6e3-4c73-b85e-9b20ba5cde45"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you should be able to see, these results are not that good. Let's try a slightly more sophisticated model."
      ],
      "metadata": {
        "id": "xiNaEgLIloeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Non-diagonal Gaussian model\n",
        "\n",
        "The second model uses a non-diagonal covariance matrix $\\bf{C}$ in the multivariate Gaussian distribution. In the next cell, calculate the average and covariance matrix over several batches of latent codes.\n",
        "\n",
        "__NB__ You can use the ```torch.cov``` function. Make sure to put the data in the right format for this (see documentation : https://pytorch.org/docs/stable/generated/torch.cov.html)."
      ],
      "metadata": {
        "id": "WjVPfkRKYMSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_batches = np.floor( len(mnist_train_loader.dataset.indices)/batch_size ).astype(int)\n",
        "\n",
        "z_average = torch.zeros(n_batches,ae_model.z_dim)\n",
        "z_covariance = torch.zeros(n_batches,ae_model.z_dim,ae_model.z_dim)\n",
        "\n",
        "for batch_idx, (data, _) in enumerate(mnist_train_loader):\n",
        "  z = ae_model.encoder(data)\n",
        "  z_average[batch_idx,:] = z.mean(dim=0)\n",
        "  z_covariance[batch_idx,:,:] = torch.cov(z.T)\n",
        "\n",
        "z_average = z_average.mean(dim=0)\n",
        "z_covariance = z_covariance.mean(dim=0)\n",
        "\n",
        "print(\"Average of latent codes:\",z_average)\n",
        "print(\"Covariance matrix of latent codes:\",z_covariance)\n"
      ],
      "metadata": {
        "id": "ArXgre39CD2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa1185fd-fffd-4b50-b7ef-4aa1e40ee60c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average of latent codes: tensor([  4.0977,  -4.4258,   1.7776,  -1.3507,   1.7013,   5.3800, -10.4796,\n",
            "         -8.4603,   8.3819,   4.5346], grad_fn=<MeanBackward1>)\n",
            "Covariance matrix of latent codes: tensor([[ 57.9592, -18.3943,  21.9091,   2.9894,  -9.3217,   1.1728, -29.3596,\n",
            "         -23.6585,  -5.9055,  -2.2811],\n",
            "        [-18.3943,  88.6507, -19.3277,  -9.8288, -24.9571, -32.8058,  45.0878,\n",
            "           7.2928,  -9.5774,  -8.2128],\n",
            "        [ 21.9091, -19.3277,  71.4595,  16.4681,  13.3173,   1.9678,  -9.9079,\n",
            "          -6.7715,   8.0912,  17.1070],\n",
            "        [  2.9894,  -9.8288,  16.4681,  72.0651,  -9.5013, -12.8231,  14.5638,\n",
            "           4.9037,  -4.7474,  -4.2930],\n",
            "        [ -9.3217, -24.9571,  13.3173,  -9.5013,  72.9896, -12.1132,   3.5019,\n",
            "         -17.5556,   0.1908,  26.9206],\n",
            "        [  1.1728, -32.8058,   1.9678, -12.8231, -12.1132,  57.4950, -28.1654,\n",
            "          -3.6147,  13.1900,   1.7593],\n",
            "        [-29.3596,  45.0878,  -9.9079,  14.5638,   3.5019, -28.1654,  84.8882,\n",
            "           9.0235,  -1.9067,   1.0854],\n",
            "        [-23.6585,   7.2928,  -6.7715,   4.9037, -17.5556,  -3.6147,   9.0235,\n",
            "          61.7252,  -3.7671, -17.2288],\n",
            "        [ -5.9055,  -9.5774,   8.0912,  -4.7474,   0.1908,  13.1900,  -1.9067,\n",
            "          -3.7671,  49.0721,   4.6817],\n",
            "        [ -2.2811,  -8.2128,  17.1070,  -4.2930,  26.9206,   1.7593,   1.0854,\n",
            "         -17.2288,   4.6817,  29.7174]], grad_fn=<MeanBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, generate some samples with this distribution. In this case, you will actually have to calculate the Cholesky decomposition and find $\\bf{L}$. For this, you can use \n",
        "\n",
        "- ```torch.linalg.cholesky```\n",
        "\n",
        "In this model, you will need to carry out matrix multiplication over a batch of latent codes, which is a bit more complicated than the previous naïve model (which used element-wise vector multiplication). So you have two options:\n",
        "\n",
        "- copy the matrix $\\bf{L}$ several times and carry out batch matrix multiplication\n",
        "- simply loop and carry out normal matrix multiplication to produce each image (this has the disadvantage of not taking advantage of any parallelisation, but it should not matter too much).\n",
        "\n",
        "In the first case, you can use the following functions:\n",
        "\n",
        "- ```torch.bmm```\n",
        "- ```torch.tile```\n",
        "\n",
        "Fill in the function to generate images using this model now:"
      ],
      "metadata": {
        "id": "JhXU8cnTZ0E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images_non_diagonal_gaussian(ae_model, z_average, z_covariance, n_images=5):\n",
        "    # Calculate Cholesky decomposition of covariance matrix: C = L L^T\n",
        "    L = torch.linalg.cholesky(z_covariance)\n",
        "\n",
        "    # Generate random noise from a standard normal distribution\n",
        "    epsilon = torch.randn(n_images, ae_model.z_dim)\n",
        "\n",
        "    # Transform noise to latent space using mean and covariance of training set\n",
        "    z_generated = z_average + torch.matmul(L, epsilon.T).T\n",
        "\n",
        "    # Transform samples to input space using decoder network\n",
        "    imgs_generated = ae_model.decoder(z_generated)\n",
        "\n",
        "    return imgs_generated\n"
      ],
      "metadata": {
        "id": "zXGlJTZ7Z4ed"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see some improvement, but we can do better than this. Thus, we turn to the variational autoencoder."
      ],
      "metadata": {
        "id": "qLtsdri6zKEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imgs_generated = generate_images_non_diagonal_gaussian(ae_model, z_average, z_covariance, n_images=5)\n",
        "display_images(imgs_generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "jJtQGzajMvba",
        "outputId": "d07821be-a950-4f8b-e57c-3d34e180e08d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3/ Variational autoencoder\n",
        "\n",
        "Now, we are going to create an variational autoencoder to carry out __image generation__. Let's first recall the idea of a variational autoencoder\n",
        "\n",
        "## Main idea\n",
        "\n",
        "The main idea is to create an autoencoder whose latent codes follow a certain distribution (a Gaussian distribution in practice). This is done with two tools : \n",
        "\n",
        "- A specific architecture, where the encoder produces the average and variance of the latent codes\n",
        "- A specially designed loss function\n",
        "\n",
        "Once the VAE is trained, it is possible to sample in the latent space by producing random normal variables and simply decoding.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "The architecture of the VAE model is as follows:\n",
        "\n",
        "The encoder consists of:\n",
        "\n",
        "Encoder :\n",
        "- Flatten input\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer (no non-linarity) to produce the average, Dense layer (no non-linarity) to produce the variance (these last two layers are in parallel)\n",
        "\n",
        "Decoder :\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer $+$ Sigmoid Activation\n",
        "- Reshape, to size $28\\times 28\\times 1$\n",
        "\n",
        "\n",
        "## Variational Autoencoder loss\n",
        "\n",
        "Recall that for the VAE, the loss function is in fact a function to __maximise__. In fact, for implementation, you will see that it is easier to __minimise__ $-\\mathcal{L}$.\n",
        "\n",
        "In the case of an image which is represented by a set of __Bernoulli__ variables (which is relevant for mnist), the original loss function (to maximise) is written :\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{L} &= \\log\\left(p_\\theta(x|z)\\right) - KL\\left( q_\\phi(z|x) \\; || \\; p_\\theta(z)\\right) \\\\\n",
        "    &= \\left(\\sum_{i} x_i \\log y_i + (1-x_i) \\log (1-y_i)\\right) - \\left(\\frac{1}{2} \\sum_j \\left( \\sigma_j^2 + \\mu_j^2 - \\log \\sigma_j^2 -1 \\right)\\right)\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "where $i$ is summed over the image pixels, and $j$ is summed over the elements of the latent space. $\\sigma_j^2$ is the $j$th element of the latent space variance, and $\\mu_j$ is the $j$th element of the latent space mean.\n",
        "\n",
        "The left part of the loss (reconstruction error) can be implemented simply as the binary cross-entropy between the input x and the output y. Since we are __maximising__ $-$[binary cross-entropy] (look at the formula), this is equivalent to minimising the binary cross-entropy.\n",
        "\n",
        "For the right part of the equation (KL divergence), you need to implement it manually. \n",
        "\n",
        "The final loss is the average, over the batch size, of the sum of the reconstruction error (left part) and the KL divergence (right part). Be careful, in the formula, the sums over $i$ and $j$ are over the number of pixels and the number of latent elements, respectively. To achieve a sum rather than an average, you can use ```torch.nn.BCELoss(reduction='sum')()```, and the ```torch.sum()``` functions.\n",
        "\n",
        "As in the case of the normal autoencoder, you will need to flatten and then reshape the tensors at the beginning/end of the network."
      ],
      "metadata": {
        "id": "8UqeNhuSdnDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(torch.nn.Module ):\n",
        "  def __init__(self, x_dim, h_dim1, h_dim2, z_dim,n_rows,n_cols,n_channels):\n",
        "    super(VAE, self).__init__()\n",
        "\n",
        "    self.n_rows = n_rows\n",
        "    self.n_cols = n_cols\n",
        "    self.n_channels = n_channels\n",
        "    self.n_pixels = (self.n_rows)*(self.n_cols)\n",
        "    self.z_dim = z_dim\n",
        "\n",
        "    # encoder part\n",
        "    self.fc1 = nn.Linear(n_pixels,h_dim1) # FILL IN CODE HERE\n",
        "    self.fc2 = nn.Linear(h_dim1, h_dim2) # FILL IN CODE HERE\n",
        "    self.fc31 = nn.Linear(h_dim2, z_dim) # FILL IN CODE HERE\n",
        "    self.fc32 = nn.Linear(h_dim2, z_dim) # FILL IN CODE HERE\n",
        "    # decoder part\n",
        "    self.fc4 = nn.Linear(z_dim, h_dim2) # FILL IN CODE HERE\n",
        "    self.fc5 = nn.Linear(h_dim2, h_dim1) # FILL IN CODE HERE\n",
        "    self.fc6 = nn.Linear(h_dim1, n_pixels) # FILL IN CODE HERE\n",
        "\n",
        "  def encoder(self, x):\n",
        "    h = x.view((-1, n_pixels))# FILL IN CODE HERE\n",
        "    h = nn.ReLU()(self.fc1(h))\n",
        "    h = nn.ReLU()(self.fc2(h))\n",
        "    return self.fc31(h), self.fc32(h)  # FILL IN CODE HERE # mu, log_var\n",
        "  \n",
        "  def decoder(self, z):\n",
        "    h = nn.ReLU()(self.fc4(z)) # FILL IN CODE HERE\n",
        "    h = nn.ReLU()(self.fc5(h)) # FILL IN CODE HERE\n",
        "    h = nn.Sigmoid()(self.fc6(h))\n",
        "    return h.view(-1, self.n_channels, self.n_rows, self.n_cols) # FILL IN CODE HERE\n",
        "\n",
        "  def sampling(self, mu, log_var):\n",
        "    # this function samples a Gaussian distribution, with average (mu) and standard deviation specified (using log_var)\n",
        "    std = torch.exp(0.5*log_var) # FILL IN CODE HERE\n",
        "    eps = torch.randn_like(std) # FILL IN CODE HERE\n",
        "    return eps.mul(std).add_(mu) # return z sample\n",
        "\n",
        "  def forward(self, x):\n",
        "    z_mu, z_log_var = self.encoder(x.view(-1, n_channels * n_rows * n_cols))\n",
        "    z = self.sampling(z_mu, z_log_var)\n",
        "    return self.decoder(z),z_mu, z_log_var\n",
        "\n",
        "  def loss_function(self,x, y, mu, log_var):\n",
        "    reconstruction_error = torch.nn.BCELoss(reduction='sum')(y, x) # FILL IN CODE HERE\n",
        "    var = torch.exp(log_var)\n",
        "    # KLD = -0.5 * torch.sum(var + mu.pow(2) - log_var -1) # FILL IN CODE HERE\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) \n",
        "    return (reconstruction_error + KLD) / x.shape[0] # FILL IN CODE HERE"
      ],
      "metadata": {
        "id": "6siMHQLheM4T"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, create the model (similarly as above)"
      ],
      "metadata": {
        "id": "hk_9fDIphlsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create model\n",
        "vae_dim_1 = ae_dim_1\n",
        "vae_dim_2 = ae_dim_2\n",
        "vae_model = VAE(x_dim=n_pixels, h_dim1= vae_dim_1, h_dim2=vae_dim_2, z_dim=z_dim,n_rows=n_rows,n_cols=n_cols,n_channels=n_channels)\n",
        "vae_optimizer = optim.Adam(vae_model.parameters())"
      ],
      "metadata": {
        "id": "pVlpC2R3htyU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, train the model. First modify the training function to the case of the vae."
      ],
      "metadata": {
        "id": "NYKLF_oMh5HO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vae(vae_model,data_train_loader,epoch):\n",
        "  train_loss = 0\n",
        "  for batch_idx, (data, _) in enumerate(data_train_loader):\n",
        "    vae_optimizer.zero_grad()\n",
        "\n",
        "    y, z_mu, z_log_var = vae_model.forward(data) # FILL IN CODE HERE\n",
        "    loss_vae = vae_model.loss_function(data, y, z_mu, z_log_var) # FILL IN CODE HERE\n",
        "    loss_vae.backward()\n",
        "    train_loss += loss_vae.item()\n",
        "    vae_optimizer.step() \n",
        "\t\t\n",
        "    if batch_idx % 100 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "      epoch, batch_idx * len(data), len(data_train_loader.dataset),\n",
        "      100. * batch_idx / len(data_train_loader), loss_vae.item() / len(data)))\n",
        "  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(data_train_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "z6DjKTWmmssb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now train the model\n",
        "for epoch in range(0, n_epochs):\n",
        "  train_vae(vae_model,mnist_train_loader,epoch)"
      ],
      "metadata": {
        "id": "L9JUUs6Kh8HB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7155a99-1924-4eba-fc0f-9b673ee94795"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/1000 (0%)]\tLoss: 4.251297\n",
            "====> Epoch: 0 Average loss: 3.1941\n",
            "Train Epoch: 1 [0/1000 (0%)]\tLoss: 2.343621\n",
            "====> Epoch: 1 Average loss: 1.8194\n",
            "Train Epoch: 2 [0/1000 (0%)]\tLoss: 1.765084\n",
            "====> Epoch: 2 Average loss: 1.5455\n",
            "Train Epoch: 3 [0/1000 (0%)]\tLoss: 1.656979\n",
            "====> Epoch: 3 Average loss: 1.4934\n",
            "Train Epoch: 4 [0/1000 (0%)]\tLoss: 1.565436\n",
            "====> Epoch: 4 Average loss: 1.4636\n",
            "Train Epoch: 5 [0/1000 (0%)]\tLoss: 1.693572\n",
            "====> Epoch: 5 Average loss: 1.4562\n",
            "Train Epoch: 6 [0/1000 (0%)]\tLoss: 1.615533\n",
            "====> Epoch: 6 Average loss: 1.4466\n",
            "Train Epoch: 7 [0/1000 (0%)]\tLoss: 1.635848\n",
            "====> Epoch: 7 Average loss: 1.4383\n",
            "Train Epoch: 8 [0/1000 (0%)]\tLoss: 1.621445\n",
            "====> Epoch: 8 Average loss: 1.4264\n",
            "Train Epoch: 9 [0/1000 (0%)]\tLoss: 1.584899\n",
            "====> Epoch: 9 Average loss: 1.4159\n",
            "Train Epoch: 10 [0/1000 (0%)]\tLoss: 1.585012\n",
            "====> Epoch: 10 Average loss: 1.4069\n",
            "Train Epoch: 11 [0/1000 (0%)]\tLoss: 1.558985\n",
            "====> Epoch: 11 Average loss: 1.3934\n",
            "Train Epoch: 12 [0/1000 (0%)]\tLoss: 1.580047\n",
            "====> Epoch: 12 Average loss: 1.3779\n",
            "Train Epoch: 13 [0/1000 (0%)]\tLoss: 1.498234\n",
            "====> Epoch: 13 Average loss: 1.3742\n",
            "Train Epoch: 14 [0/1000 (0%)]\tLoss: 1.520945\n",
            "====> Epoch: 14 Average loss: 1.3563\n",
            "Train Epoch: 15 [0/1000 (0%)]\tLoss: 1.521462\n",
            "====> Epoch: 15 Average loss: 1.3443\n",
            "Train Epoch: 16 [0/1000 (0%)]\tLoss: 1.466218\n",
            "====> Epoch: 16 Average loss: 1.3426\n",
            "Train Epoch: 17 [0/1000 (0%)]\tLoss: 1.440501\n",
            "====> Epoch: 17 Average loss: 1.3139\n",
            "Train Epoch: 18 [0/1000 (0%)]\tLoss: 1.448933\n",
            "====> Epoch: 18 Average loss: 1.3117\n",
            "Train Epoch: 19 [0/1000 (0%)]\tLoss: 1.429165\n",
            "====> Epoch: 19 Average loss: 1.2855\n",
            "Train Epoch: 20 [0/1000 (0%)]\tLoss: 1.434220\n",
            "====> Epoch: 20 Average loss: 1.2835\n",
            "Train Epoch: 21 [0/1000 (0%)]\tLoss: 1.397244\n",
            "====> Epoch: 21 Average loss: 1.2631\n",
            "Train Epoch: 22 [0/1000 (0%)]\tLoss: 1.410465\n",
            "====> Epoch: 22 Average loss: 1.2557\n",
            "Train Epoch: 23 [0/1000 (0%)]\tLoss: 1.393992\n",
            "====> Epoch: 23 Average loss: 1.2375\n",
            "Train Epoch: 24 [0/1000 (0%)]\tLoss: 1.353552\n",
            "====> Epoch: 24 Average loss: 1.2200\n",
            "Train Epoch: 25 [0/1000 (0%)]\tLoss: 1.335829\n",
            "====> Epoch: 25 Average loss: 1.2066\n",
            "Train Epoch: 26 [0/1000 (0%)]\tLoss: 1.386193\n",
            "====> Epoch: 26 Average loss: 1.1912\n",
            "Train Epoch: 27 [0/1000 (0%)]\tLoss: 1.329433\n",
            "====> Epoch: 27 Average loss: 1.1689\n",
            "Train Epoch: 28 [0/1000 (0%)]\tLoss: 1.281866\n",
            "====> Epoch: 28 Average loss: 1.1582\n",
            "Train Epoch: 29 [0/1000 (0%)]\tLoss: 1.281348\n",
            "====> Epoch: 29 Average loss: 1.1416\n",
            "Train Epoch: 30 [0/1000 (0%)]\tLoss: 1.254614\n",
            "====> Epoch: 30 Average loss: 1.1174\n",
            "Train Epoch: 31 [0/1000 (0%)]\tLoss: 1.204627\n",
            "====> Epoch: 31 Average loss: 1.1052\n",
            "Train Epoch: 32 [0/1000 (0%)]\tLoss: 1.170525\n",
            "====> Epoch: 32 Average loss: 1.0963\n",
            "Train Epoch: 33 [0/1000 (0%)]\tLoss: 1.200039\n",
            "====> Epoch: 33 Average loss: 1.0845\n",
            "Train Epoch: 34 [0/1000 (0%)]\tLoss: 1.212134\n",
            "====> Epoch: 34 Average loss: 1.0718\n",
            "Train Epoch: 35 [0/1000 (0%)]\tLoss: 1.189841\n",
            "====> Epoch: 35 Average loss: 1.0603\n",
            "Train Epoch: 36 [0/1000 (0%)]\tLoss: 1.159059\n",
            "====> Epoch: 36 Average loss: 1.0539\n",
            "Train Epoch: 37 [0/1000 (0%)]\tLoss: 1.136330\n",
            "====> Epoch: 37 Average loss: 1.0388\n",
            "Train Epoch: 38 [0/1000 (0%)]\tLoss: 1.152247\n",
            "====> Epoch: 38 Average loss: 1.0336\n",
            "Train Epoch: 39 [0/1000 (0%)]\tLoss: 1.128673\n",
            "====> Epoch: 39 Average loss: 1.0297\n",
            "Train Epoch: 40 [0/1000 (0%)]\tLoss: 1.149730\n",
            "====> Epoch: 40 Average loss: 1.0181\n",
            "Train Epoch: 41 [0/1000 (0%)]\tLoss: 1.154158\n",
            "====> Epoch: 41 Average loss: 1.0171\n",
            "Train Epoch: 42 [0/1000 (0%)]\tLoss: 1.155675\n",
            "====> Epoch: 42 Average loss: 1.0106\n",
            "Train Epoch: 43 [0/1000 (0%)]\tLoss: 1.111051\n",
            "====> Epoch: 43 Average loss: 0.9933\n",
            "Train Epoch: 44 [0/1000 (0%)]\tLoss: 1.110388\n",
            "====> Epoch: 44 Average loss: 0.9946\n",
            "Train Epoch: 45 [0/1000 (0%)]\tLoss: 1.099826\n",
            "====> Epoch: 45 Average loss: 0.9878\n",
            "Train Epoch: 46 [0/1000 (0%)]\tLoss: 1.081126\n",
            "====> Epoch: 46 Average loss: 0.9783\n",
            "Train Epoch: 47 [0/1000 (0%)]\tLoss: 1.071768\n",
            "====> Epoch: 47 Average loss: 0.9727\n",
            "Train Epoch: 48 [0/1000 (0%)]\tLoss: 1.087567\n",
            "====> Epoch: 48 Average loss: 0.9722\n",
            "Train Epoch: 49 [0/1000 (0%)]\tLoss: 1.125629\n",
            "====> Epoch: 49 Average loss: 0.9654\n",
            "Train Epoch: 50 [0/1000 (0%)]\tLoss: 1.081000\n",
            "====> Epoch: 50 Average loss: 0.9627\n",
            "Train Epoch: 51 [0/1000 (0%)]\tLoss: 1.008379\n",
            "====> Epoch: 51 Average loss: 0.9506\n",
            "Train Epoch: 52 [0/1000 (0%)]\tLoss: 1.075900\n",
            "====> Epoch: 52 Average loss: 0.9478\n",
            "Train Epoch: 53 [0/1000 (0%)]\tLoss: 1.092116\n",
            "====> Epoch: 53 Average loss: 0.9541\n",
            "Train Epoch: 54 [0/1000 (0%)]\tLoss: 1.004102\n",
            "====> Epoch: 54 Average loss: 0.9417\n",
            "Train Epoch: 55 [0/1000 (0%)]\tLoss: 0.996247\n",
            "====> Epoch: 55 Average loss: 0.9281\n",
            "Train Epoch: 56 [0/1000 (0%)]\tLoss: 1.016532\n",
            "====> Epoch: 56 Average loss: 0.9296\n",
            "Train Epoch: 57 [0/1000 (0%)]\tLoss: 1.012508\n",
            "====> Epoch: 57 Average loss: 0.9236\n",
            "Train Epoch: 58 [0/1000 (0%)]\tLoss: 0.987540\n",
            "====> Epoch: 58 Average loss: 0.9208\n",
            "Train Epoch: 59 [0/1000 (0%)]\tLoss: 1.000644\n",
            "====> Epoch: 59 Average loss: 0.9235\n",
            "Train Epoch: 60 [0/1000 (0%)]\tLoss: 1.007998\n",
            "====> Epoch: 60 Average loss: 0.9122\n",
            "Train Epoch: 61 [0/1000 (0%)]\tLoss: 1.022502\n",
            "====> Epoch: 61 Average loss: 0.9079\n",
            "Train Epoch: 62 [0/1000 (0%)]\tLoss: 1.036010\n",
            "====> Epoch: 62 Average loss: 0.9058\n",
            "Train Epoch: 63 [0/1000 (0%)]\tLoss: 0.984172\n",
            "====> Epoch: 63 Average loss: 0.8983\n",
            "Train Epoch: 64 [0/1000 (0%)]\tLoss: 0.989241\n",
            "====> Epoch: 64 Average loss: 0.9011\n",
            "Train Epoch: 65 [0/1000 (0%)]\tLoss: 0.970803\n",
            "====> Epoch: 65 Average loss: 0.8958\n",
            "Train Epoch: 66 [0/1000 (0%)]\tLoss: 0.955835\n",
            "====> Epoch: 66 Average loss: 0.8871\n",
            "Train Epoch: 67 [0/1000 (0%)]\tLoss: 1.020713\n",
            "====> Epoch: 67 Average loss: 0.8884\n",
            "Train Epoch: 68 [0/1000 (0%)]\tLoss: 0.983283\n",
            "====> Epoch: 68 Average loss: 0.8887\n",
            "Train Epoch: 69 [0/1000 (0%)]\tLoss: 1.007347\n",
            "====> Epoch: 69 Average loss: 0.8795\n",
            "Train Epoch: 70 [0/1000 (0%)]\tLoss: 0.983733\n",
            "====> Epoch: 70 Average loss: 0.8770\n",
            "Train Epoch: 71 [0/1000 (0%)]\tLoss: 0.942740\n",
            "====> Epoch: 71 Average loss: 0.8785\n",
            "Train Epoch: 72 [0/1000 (0%)]\tLoss: 1.014388\n",
            "====> Epoch: 72 Average loss: 0.8739\n",
            "Train Epoch: 73 [0/1000 (0%)]\tLoss: 0.969209\n",
            "====> Epoch: 73 Average loss: 0.8710\n",
            "Train Epoch: 74 [0/1000 (0%)]\tLoss: 0.966700\n",
            "====> Epoch: 74 Average loss: 0.8656\n",
            "Train Epoch: 75 [0/1000 (0%)]\tLoss: 0.963738\n",
            "====> Epoch: 75 Average loss: 0.8685\n",
            "Train Epoch: 76 [0/1000 (0%)]\tLoss: 0.960737\n",
            "====> Epoch: 76 Average loss: 0.8685\n",
            "Train Epoch: 77 [0/1000 (0%)]\tLoss: 0.952507\n",
            "====> Epoch: 77 Average loss: 0.8580\n",
            "Train Epoch: 78 [0/1000 (0%)]\tLoss: 0.958048\n",
            "====> Epoch: 78 Average loss: 0.8537\n",
            "Train Epoch: 79 [0/1000 (0%)]\tLoss: 0.931318\n",
            "====> Epoch: 79 Average loss: 0.8541\n",
            "Train Epoch: 80 [0/1000 (0%)]\tLoss: 0.943808\n",
            "====> Epoch: 80 Average loss: 0.8499\n",
            "Train Epoch: 81 [0/1000 (0%)]\tLoss: 0.946911\n",
            "====> Epoch: 81 Average loss: 0.8451\n",
            "Train Epoch: 82 [0/1000 (0%)]\tLoss: 0.938437\n",
            "====> Epoch: 82 Average loss: 0.8444\n",
            "Train Epoch: 83 [0/1000 (0%)]\tLoss: 0.953918\n",
            "====> Epoch: 83 Average loss: 0.8428\n",
            "Train Epoch: 84 [0/1000 (0%)]\tLoss: 0.955064\n",
            "====> Epoch: 84 Average loss: 0.8413\n",
            "Train Epoch: 85 [0/1000 (0%)]\tLoss: 0.888112\n",
            "====> Epoch: 85 Average loss: 0.8358\n",
            "Train Epoch: 86 [0/1000 (0%)]\tLoss: 0.920837\n",
            "====> Epoch: 86 Average loss: 0.8328\n",
            "Train Epoch: 87 [0/1000 (0%)]\tLoss: 0.912043\n",
            "====> Epoch: 87 Average loss: 0.8323\n",
            "Train Epoch: 88 [0/1000 (0%)]\tLoss: 0.920894\n",
            "====> Epoch: 88 Average loss: 0.8320\n",
            "Train Epoch: 89 [0/1000 (0%)]\tLoss: 0.911998\n",
            "====> Epoch: 89 Average loss: 0.8248\n",
            "Train Epoch: 90 [0/1000 (0%)]\tLoss: 0.927584\n",
            "====> Epoch: 90 Average loss: 0.8281\n",
            "Train Epoch: 91 [0/1000 (0%)]\tLoss: 0.927435\n",
            "====> Epoch: 91 Average loss: 0.8189\n",
            "Train Epoch: 92 [0/1000 (0%)]\tLoss: 0.923299\n",
            "====> Epoch: 92 Average loss: 0.8226\n",
            "Train Epoch: 93 [0/1000 (0%)]\tLoss: 0.895887\n",
            "====> Epoch: 93 Average loss: 0.8210\n",
            "Train Epoch: 94 [0/1000 (0%)]\tLoss: 0.900821\n",
            "====> Epoch: 94 Average loss: 0.8101\n",
            "Train Epoch: 95 [0/1000 (0%)]\tLoss: 0.908916\n",
            "====> Epoch: 95 Average loss: 0.8127\n",
            "Train Epoch: 96 [0/1000 (0%)]\tLoss: 0.897607\n",
            "====> Epoch: 96 Average loss: 0.8095\n",
            "Train Epoch: 97 [0/1000 (0%)]\tLoss: 0.888224\n",
            "====> Epoch: 97 Average loss: 0.8072\n",
            "Train Epoch: 98 [0/1000 (0%)]\tLoss: 0.909094\n",
            "====> Epoch: 98 Average loss: 0.8056\n",
            "Train Epoch: 99 [0/1000 (0%)]\tLoss: 0.902473\n",
            "====> Epoch: 99 Average loss: 0.8040\n",
            "Train Epoch: 100 [0/1000 (0%)]\tLoss: 0.874825\n",
            "====> Epoch: 100 Average loss: 0.8004\n",
            "Train Epoch: 101 [0/1000 (0%)]\tLoss: 0.883250\n",
            "====> Epoch: 101 Average loss: 0.7983\n",
            "Train Epoch: 102 [0/1000 (0%)]\tLoss: 0.879635\n",
            "====> Epoch: 102 Average loss: 0.7976\n",
            "Train Epoch: 103 [0/1000 (0%)]\tLoss: 0.880134\n",
            "====> Epoch: 103 Average loss: 0.7921\n",
            "Train Epoch: 104 [0/1000 (0%)]\tLoss: 0.880700\n",
            "====> Epoch: 104 Average loss: 0.7868\n",
            "Train Epoch: 105 [0/1000 (0%)]\tLoss: 0.876291\n",
            "====> Epoch: 105 Average loss: 0.7876\n",
            "Train Epoch: 106 [0/1000 (0%)]\tLoss: 0.904417\n",
            "====> Epoch: 106 Average loss: 0.7856\n",
            "Train Epoch: 107 [0/1000 (0%)]\tLoss: 0.855024\n",
            "====> Epoch: 107 Average loss: 0.7853\n",
            "Train Epoch: 108 [0/1000 (0%)]\tLoss: 0.875244\n",
            "====> Epoch: 108 Average loss: 0.7865\n",
            "Train Epoch: 109 [0/1000 (0%)]\tLoss: 0.916553\n",
            "====> Epoch: 109 Average loss: 0.7825\n",
            "Train Epoch: 110 [0/1000 (0%)]\tLoss: 0.830903\n",
            "====> Epoch: 110 Average loss: 0.7819\n",
            "Train Epoch: 111 [0/1000 (0%)]\tLoss: 0.871519\n",
            "====> Epoch: 111 Average loss: 0.7800\n",
            "Train Epoch: 112 [0/1000 (0%)]\tLoss: 0.888983\n",
            "====> Epoch: 112 Average loss: 0.7764\n",
            "Train Epoch: 113 [0/1000 (0%)]\tLoss: 0.834754\n",
            "====> Epoch: 113 Average loss: 0.7720\n",
            "Train Epoch: 114 [0/1000 (0%)]\tLoss: 0.855521\n",
            "====> Epoch: 114 Average loss: 0.7734\n",
            "Train Epoch: 115 [0/1000 (0%)]\tLoss: 0.864806\n",
            "====> Epoch: 115 Average loss: 0.7689\n",
            "Train Epoch: 116 [0/1000 (0%)]\tLoss: 0.823714\n",
            "====> Epoch: 116 Average loss: 0.7634\n",
            "Train Epoch: 117 [0/1000 (0%)]\tLoss: 0.859636\n",
            "====> Epoch: 117 Average loss: 0.7709\n",
            "Train Epoch: 118 [0/1000 (0%)]\tLoss: 0.823015\n",
            "====> Epoch: 118 Average loss: 0.7605\n",
            "Train Epoch: 119 [0/1000 (0%)]\tLoss: 0.869462\n",
            "====> Epoch: 119 Average loss: 0.7602\n",
            "Train Epoch: 120 [0/1000 (0%)]\tLoss: 0.811253\n",
            "====> Epoch: 120 Average loss: 0.7593\n",
            "Train Epoch: 121 [0/1000 (0%)]\tLoss: 0.844840\n",
            "====> Epoch: 121 Average loss: 0.7613\n",
            "Train Epoch: 122 [0/1000 (0%)]\tLoss: 0.824578\n",
            "====> Epoch: 122 Average loss: 0.7592\n",
            "Train Epoch: 123 [0/1000 (0%)]\tLoss: 0.826566\n",
            "====> Epoch: 123 Average loss: 0.7546\n",
            "Train Epoch: 124 [0/1000 (0%)]\tLoss: 0.830271\n",
            "====> Epoch: 124 Average loss: 0.7527\n",
            "Train Epoch: 125 [0/1000 (0%)]\tLoss: 0.823422\n",
            "====> Epoch: 125 Average loss: 0.7549\n",
            "Train Epoch: 126 [0/1000 (0%)]\tLoss: 0.789070\n",
            "====> Epoch: 126 Average loss: 0.7464\n",
            "Train Epoch: 127 [0/1000 (0%)]\tLoss: 0.817586\n",
            "====> Epoch: 127 Average loss: 0.7474\n",
            "Train Epoch: 128 [0/1000 (0%)]\tLoss: 0.865320\n",
            "====> Epoch: 128 Average loss: 0.7479\n",
            "Train Epoch: 129 [0/1000 (0%)]\tLoss: 0.838406\n",
            "====> Epoch: 129 Average loss: 0.7470\n",
            "Train Epoch: 130 [0/1000 (0%)]\tLoss: 0.831618\n",
            "====> Epoch: 130 Average loss: 0.7485\n",
            "Train Epoch: 131 [0/1000 (0%)]\tLoss: 0.857433\n",
            "====> Epoch: 131 Average loss: 0.7463\n",
            "Train Epoch: 132 [0/1000 (0%)]\tLoss: 0.830913\n",
            "====> Epoch: 132 Average loss: 0.7453\n",
            "Train Epoch: 133 [0/1000 (0%)]\tLoss: 0.831820\n",
            "====> Epoch: 133 Average loss: 0.7405\n",
            "Train Epoch: 134 [0/1000 (0%)]\tLoss: 0.840190\n",
            "====> Epoch: 134 Average loss: 0.7434\n",
            "Train Epoch: 135 [0/1000 (0%)]\tLoss: 0.840475\n",
            "====> Epoch: 135 Average loss: 0.7388\n",
            "Train Epoch: 136 [0/1000 (0%)]\tLoss: 0.837124\n",
            "====> Epoch: 136 Average loss: 0.7320\n",
            "Train Epoch: 137 [0/1000 (0%)]\tLoss: 0.796176\n",
            "====> Epoch: 137 Average loss: 0.7381\n",
            "Train Epoch: 138 [0/1000 (0%)]\tLoss: 0.785825\n",
            "====> Epoch: 138 Average loss: 0.7301\n",
            "Train Epoch: 139 [0/1000 (0%)]\tLoss: 0.802022\n",
            "====> Epoch: 139 Average loss: 0.7307\n",
            "Train Epoch: 140 [0/1000 (0%)]\tLoss: 0.803175\n",
            "====> Epoch: 140 Average loss: 0.7277\n",
            "Train Epoch: 141 [0/1000 (0%)]\tLoss: 0.813980\n",
            "====> Epoch: 141 Average loss: 0.7288\n",
            "Train Epoch: 142 [0/1000 (0%)]\tLoss: 0.804904\n",
            "====> Epoch: 142 Average loss: 0.7294\n",
            "Train Epoch: 143 [0/1000 (0%)]\tLoss: 0.800979\n",
            "====> Epoch: 143 Average loss: 0.7266\n",
            "Train Epoch: 144 [0/1000 (0%)]\tLoss: 0.791766\n",
            "====> Epoch: 144 Average loss: 0.7290\n",
            "Train Epoch: 145 [0/1000 (0%)]\tLoss: 0.796937\n",
            "====> Epoch: 145 Average loss: 0.7266\n",
            "Train Epoch: 146 [0/1000 (0%)]\tLoss: 0.785397\n",
            "====> Epoch: 146 Average loss: 0.7231\n",
            "Train Epoch: 147 [0/1000 (0%)]\tLoss: 0.785087\n",
            "====> Epoch: 147 Average loss: 0.7223\n",
            "Train Epoch: 148 [0/1000 (0%)]\tLoss: 0.822449\n",
            "====> Epoch: 148 Average loss: 0.7235\n",
            "Train Epoch: 149 [0/1000 (0%)]\tLoss: 0.805269\n",
            "====> Epoch: 149 Average loss: 0.7220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, generate some images with the VAE model"
      ],
      "metadata": {
        "id": "uTfRje_AkKDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images_vae(vae_model, n_images=5):\n",
        "    epsilon = torch.randn(n_images, vae_model.z_dim)\n",
        "    imgs_generated = vae_model.decoder(epsilon)\n",
        "    return imgs_generated\n",
        "\n",
        "imgs_generated = generate_images_vae(vae_model, n_images=5)\n",
        "display_images(imgs_generated)\n"
      ],
      "metadata": {
        "id": "41tXdNsFkKk5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "34648e6b-84c5-4a00-d4af-e86d9311076d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you think the results are better ? What difference can you see ? What advantage does the Variational Autoencoder have over the simple autoencoder model, even though the second autoencoder approach has a more complex probabilistic latent model (a full covariance matrix) ?"
      ],
      "metadata": {
        "id": "lGnvKoynzaFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Answer__\n",
        "\n",
        "Yes, the results of the Variational Autoencoder are better than the simple autoencoder. The images generated by the VAE are sharper and clearer compared to the images generated by the simple autoencoder.\n",
        "\n",
        "The main advantage of the Variational Autoencoder over the simple autoencoder is that it introduces a probabilistic model for the latent space, which allows the VAE to generate new samples by sampling from the latent space. In contrast, the simple autoencoder only maps the input data to a lower-dimensional representation, and there is no probabilistic model involved in the latent space.\n",
        "\n",
        "The VAE's probabilistic model for the latent space makes it more flexible and powerful than the simple autoencoder, as it allows the VAE to generate new samples that are similar to the training data but not identical to it. In contrast, the simple autoencoder can only generate samples that are close to the training data, as it maps the input data to a fixed lower-dimensional representation without any probabilistic model."
      ],
      "metadata": {
        "id": "uuYn3_PBzjkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now compare the models quantitavely."
      ],
      "metadata": {
        "id": "uXm-D9Ef9vYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Evaluating and comparing the models\n",
        "\n",
        "We will evaluate the models, in the following manner:\n",
        "\n",
        "- we train a simple convolutional neural network classifier on mnist, to a good accuracy\n",
        "- we generate images with each model\n",
        "- we find the average of the highest probability of the images according to the classifier, for each model. If this value is high, it means that on average the classifier considers that the images look like a \n",
        "\n",
        "We will use the following convoluional architecture for the classifier:\n",
        "\n",
        "- conv2d, filter size  3×3 , 32 filters, stride=(2,2), padding=\"SAME\"\n",
        "- ReLU\n",
        "- conv2d, filter size  3×3 , 32 filters, stride=(2,2), padding=\"SAME\"\n",
        "- ReLU\n",
        "- MaxPool2D, stride=(2,2)\n",
        "- Flatten\n",
        "- Dense layer\n",
        "\n",
        "Now, define the model. To make things easier, use the ```torch.nn.Sequential``` API (there is no need for a Class in this simple case)."
      ],
      "metadata": {
        "id": "04MddkzuE324"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "n_epochs = 20\n",
        "batch_size = 64\n",
        "nb_classes = int(mnist_trainset.targets.max()+1)\n",
        "\n",
        "# number of convolutional filters to use\n",
        "nb_filters = 32\n",
        "# convolution kernel size\n",
        "kernel_size = (3, 3)\n",
        "# size of pooling area for max pooling\n",
        "pool_size = (2, 2)\n",
        "\n",
        "# --- Size of the successive layers\n",
        "n_h_0 = 1 #greyscale input images\n",
        "n_h_1 = nb_filters\n",
        "n_h_2 = nb_filters\n",
        "\n",
        "mnist_classification_model = torch.nn.Sequential(\n",
        "    nn.Conv2d(n_h_0, n_h_1, kernel_size=kernel_size, stride=(1, 1), padding=\"same\"),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(n_h_1, n_h_2, kernel_size=kernel_size, stride=(1, 1), padding=\"same\"),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(pool_size, stride=(2, 2)),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(14*14*n_h_2, nb_classes),\n",
        ") # FILL IN CODE HERE\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # FILL IN CODE HERE\n",
        "optimizer = torch.optim.Adam(mnist_classification_model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "P87a-DkXFOCv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to calculate accuracy, instead of loss"
      ],
      "metadata": {
        "id": "_sUZONxw2bnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(x_pred,x_label):\n",
        "  acc = torch.sum(x_pred == x_label)/(x_pred.shape[0])\n",
        "  return acc"
      ],
      "metadata": {
        "id": "aOww0ydr2fT0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, train the model. You should be able to achieve an accuracy close to 1.00 within 20 epochs"
      ],
      "metadata": {
        "id": "qw0vkZIqFcse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "for epoch in range(0,n_epochs):\n",
        "  train_loss=0.0\n",
        "\n",
        "  for batch_idx, (imgs, labels) in enumerate(mnist_train_loader):\n",
        "\n",
        "    # set the gradients back to 0\n",
        "    optimizer.zero_grad()\n",
        "    predict=mnist_classification_model(imgs)\n",
        "    # apply loss function\n",
        "    loss=criterion(predict,labels)\n",
        "    acc = get_accuracy(torch.argmax(predict,dim=1),labels)\n",
        "    # backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss=loss.item()\n",
        "  print('Epoch:{} Train Loss:{:.4f} Accuracy:{:.4f}'.format(epoch,train_loss/imgs.shape[0],acc))"
      ],
      "metadata": {
        "id": "0FA8YoX2FcHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf75add-11a0-4a1d-a98a-ae31fd2579ac"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0 Train Loss:0.0126 Accuracy:0.4844\n",
            "Epoch:1 Train Loss:0.0059 Accuracy:0.7500\n",
            "Epoch:2 Train Loss:0.0048 Accuracy:0.7812\n",
            "Epoch:3 Train Loss:0.0032 Accuracy:0.8594\n",
            "Epoch:4 Train Loss:0.0034 Accuracy:0.8438\n",
            "Epoch:5 Train Loss:0.0024 Accuracy:0.8828\n",
            "Epoch:6 Train Loss:0.0020 Accuracy:0.9062\n",
            "Epoch:7 Train Loss:0.0012 Accuracy:0.9688\n",
            "Epoch:8 Train Loss:0.0009 Accuracy:0.9688\n",
            "Epoch:9 Train Loss:0.0005 Accuracy:0.9844\n",
            "Epoch:10 Train Loss:0.0007 Accuracy:0.9844\n",
            "Epoch:11 Train Loss:0.0006 Accuracy:0.9844\n",
            "Epoch:12 Train Loss:0.0004 Accuracy:1.0000\n",
            "Epoch:13 Train Loss:0.0002 Accuracy:1.0000\n",
            "Epoch:14 Train Loss:0.0002 Accuracy:0.9922\n",
            "Epoch:15 Train Loss:0.0002 Accuracy:1.0000\n",
            "Epoch:16 Train Loss:0.0002 Accuracy:0.9922\n",
            "Epoch:17 Train Loss:0.0003 Accuracy:0.9922\n",
            "Epoch:18 Train Loss:0.0001 Accuracy:0.9922\n",
            "Epoch:19 Train Loss:0.0000 Accuracy:1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the average maximum prediction of the images generated by each generative model (higher is better)\n",
        "\n",
        "Now, we will evaluate the models. For each ones, produce a certain number of images, and put those images through the classification network. Then find the maximum class probability of each image, and average it over all the images. We will use this as a metric to evaluate each model.\n",
        "\n",
        "__CAREFUL__: the output of the network does __not__ include the Softmax layer, so you will have to carry it out, with:\n",
        "- ```torch.nn.Softmax()(...)```\n",
        "\n",
        "Define this metric now:"
      ],
      "metadata": {
        "id": "zFgN5LblFwTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generative_model_score(imgs_in,classification_model):\n",
        "  gen_score = torch.mean(torch.max(F.softmax(classification_model(imgs_in), dim=1), dim=1).values )\n",
        "  return(gen_score)"
      ],
      "metadata": {
        "id": "lCJ_0qqjOXHT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, generate some images with each of the three models, and evaluate these models:"
      ],
      "metadata": {
        "id": "yGq7YFg51UoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imgs_diagonal_gaussian = generate_images_diagonal_gaussian(ae_model,z_average,z_sigma,n_images = 50)\n",
        "imgs_non_diagonal_gaussian = generate_images_non_diagonal_gaussian(ae_model,z_average,z_covariance,n_images = 50)\n",
        "imgs_vae = generate_images_vae(vae_model,n_images=50)\n",
        "\n",
        "# average of maximum of first model \n",
        "diagonal_gaussian_score = float(generative_model_score(imgs_diagonal_gaussian,mnist_classification_model))\n",
        "non_diagonal_gaussian_score = float(generative_model_score(imgs_non_diagonal_gaussian,mnist_classification_model))\n",
        "vae_gaussian_score = float(generative_model_score(imgs_vae,mnist_classification_model))\n",
        "\n",
        "print(\"Diagonal gaussian generative model score : \",diagonal_gaussian_score)\n",
        "print(\"Non diagonal gaussian generative model score : \",non_diagonal_gaussian_score)\n",
        "print(\"Variational autoencoder model score: \",vae_gaussian_score) "
      ],
      "metadata": {
        "id": "4-L4u2jhILFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d90b61-7a74-4b3c-f187-cb04c3ac6616"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diagonal gaussian generative model score :  0.8471719622612\n",
            "Non diagonal gaussian generative model score :  0.8445268273353577\n",
            "Variational autoencoder model score:  0.889818549156189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please answer the following questions:\n",
        "\n",
        "- Which model is better quantitatively ? \n",
        "\n",
        "The variational autoencoder\n",
        "\n",
        "- Do the quantitative result support the qualitative results ?\n",
        "\n",
        "Yes\n",
        "\n",
        "- Can you see any drawbacks of this method of evaluation ?\n",
        "\n",
        "Yes, the generative model may only generate images that are highly recognizable, which may cause biased scores.\n",
        "\n",
        "- Can you propose any more sophisticated models than the multivariate Gaussian approach (apart from the variational autoencoder) ? \n",
        "\n",
        "I would propose GANs or StyleGANs"
      ],
      "metadata": {
        "id": "sxvsG8FC1gNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Answer__\n",
        "\n",
        "The answers are written above."
      ],
      "metadata": {
        "id": "3SYCyfKR3G7Y"
      }
    }
  ]
}